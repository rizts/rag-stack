[![CI/CD Pipeline](https://github.com/rizts/rag-stack/actions/workflows/deploy.yml/badge.svg)](https://github.com/rizts/rag-stack/actions/workflows/deploy.yml)

# ğŸ§  AI Knowledge Service â€” RAG & FastAPI

This project is a **Refactored and Production-Ready version** of the original [rizts/rag-ai-concept](https://github.com/rizts/rag-ai-concept).  
It aims to be more structured, modular, and ready for real-world deployment, with a clear separation between **Backend (FastAPI)** and **Frontend (Vite + ReactJS)**.

**ğŸ”— Live Demo:**
- Frontend: [Deployed on Vercel](https://your-app.vercel.app)
- Backend API: [Deployed on Railway](https://rag-stack-production.up.railway.app)

---

## ğŸš€ Overview

This project demonstrates a complete **AI pipeline** with:
- ğŸ§© **Backend (FastAPI)** â€” API layer, Gemini integration, and LangChain-based chunking.
- âš›ï¸ **Frontend (Vite + React + TypeScript)** â€” interactive RAG chat UI.
- ğŸ§  **AI Layer** â€” intelligent chunking, multiple embedding providers (Jina AI as default), semantic retrieval, and Gemini answer generation.
- ğŸ’¾ **Vector Database (Qdrant Cloud)** â€” document storage and vector similarity search.
- ğŸš€ **CI/CD** â€” Automated deployment via GitHub Actions to Railway (backend) + Vercel (frontend).

The goal is to demonstrate a production-level **Retrieval-Augmented Generation (RAG)** system that can:
1. Process documents and intelligently chunk them using **LangChain**.
2. Generate embeddings using **multiple embedding providers** with **Jina AI as the default** and store them in **Qdrant** Vector Database.
3. Expose APIs for semantic search and knowledge retrieval.
4. Integrate with modern DevOps practices to showcase AI orchestration lifecycle.

### ğŸ”„ Important Change: Multiple Embedding Providers

**Recent Update:** We've implemented support for multiple embedding providers to give you flexibility in choosing the best service for your needs. The system now supports:
- **Jina AI** (default) - Free 8000 requests/day
- **Cohere** - 1000 requests/month free  
- **Voyage AI** - 20M tokens/month free
- **HuggingFace** - Legacy option for local models

**Benefits:**
- ğŸ”„ **Flexibility** - Choose the provider that best fits your use case
- ğŸ’° **Cost optimization** - Select based on pricing and free tier options
- ğŸš€ **Performance** - Different providers offer different strengths
- ğŸ›¡ï¸ **Privacy** - Option to use local models with HuggingFace

**Configuration:**
- Provide your `JINA_API_KEY` in environment variables
- The service uses `jina-embeddings-v3` via Jina API

**Note:** If you encounter issues with the default model not being available via the feature_extraction API, 
consider using alternative models like `sentence-transformers/all-MiniLM-L6-v2` which are more reliably 
supported on the Jina Inference API.

This approach is specifically optimized for resource-constrained environments like Railway's free tier.

---

## ğŸ—ï¸ Architecture Overview

```mermaid
graph TD
  A[ğŸ“„ Document Upload] --> B[ğŸ” Intelligent Chunking LangChain]
  B --> C[ğŸ”¢ Multiple Embedding Providers (Jina AI Default)]
  C --> D[(ğŸ§  Qdrant Vector DB)]
  E[ğŸ’¬ Query Request] --> F[Semantic Retrieval + Contextual Search]
  F --> G[ğŸ§  Gemini Generative Response]
  G --> H[ğŸ’¡ Answer Output React UI]
```

---

## ğŸ—‚ï¸ Project Structure

```
rag-stack/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ deploy.yml          # CI/CD configuration
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ health.py       # Health check endpoint
â”‚   â”‚   â”‚   â””â”€â”€ rag.py          # RAG endpoints (index, query)
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py       # Centralized configuration
â”‚   â”‚   â”‚   â””â”€â”€ logger.py       # Logging setup
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings.py   # Local sentence-transformer embeddings
â”‚   â”‚   â”‚   â”œâ”€â”€ semantic.py     # Semantic search logic
â”‚   â”‚   â”‚   â””â”€â”€ vectorstore_qdrant.py      # Qdrant integration
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â””â”€â”€ chunking.py     # LangChain text chunking
â”‚   â”‚   â””â”€â”€ main.py             # FastAPI application
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â””â”€â”€ test_rag_basic.py   # Unit tests
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ requirements-dev.txt
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ rag.ts          # API client
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ AnswerCard.tsx  # Answer display
â”‚   â”‚   â”‚   â””â”€â”€ QueryForm.tsx   # Query input form
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â””â”€â”€ main.tsx
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ vite.config.ts
â”‚   â””â”€â”€ vercel.json             # Vercel configuration
â””â”€â”€ README.md
```

---

## ğŸ› ï¸ Local Development Setup

### Prerequisites

- Python 3.11+
- Node.js 20+
- Docker (for local Qdrant)
- Git

### 1. Clone Repository

```bash
git clone https://github.com/rizts/rag-stack.git
cd rag-stack
```

### 2. Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Copy environment file
cp .env.example .env

# Edit .env and add your API keys:
# - GEMINI_API_KEY
# - JINA_API_KEY
# - QDRANT_URL (use localhost for local dev)
```

### 3. Run Qdrant (Local Development)

```bash
cd backend/docker
docker compose up -d
```

Qdrant will run at `http://localhost:6333`

### 4. Run Backend

```bash
cd backend
uvicorn app.main:app --reload
```

Backend runs at `http://localhost:8000`

Check health: `http://localhost:8000/health`

### 5. Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Copy environment file
cp .env.example .env

# Edit .env:
# VITE_API_BASE=http://localhost:8000
```

### 6. Run Frontend

```bash
npm run dev
```

Frontend runs at `http://localhost:5173`

---

## ğŸŒ Production Deployment

### CI/CD Pipeline

This project uses **GitHub Actions** for automated deployment:

#### Workflow Overview

```yaml
Trigger: Push to main or Pull Request
â”œâ”€â”€ 1. Detect Changes (backend/frontend/both)
â”œâ”€â”€ 2. Run Tests
â”‚   â”œâ”€â”€ Backend: pytest
â”‚   â””â”€â”€ Frontend: lint + build
â”œâ”€â”€ 3. Deploy Backend (if backend changed)
â”‚   â””â”€â”€ Railway auto-deploys via GitHub integration
â””â”€â”€ 4. Deploy Frontend (if frontend changed)
    â””â”€â”€ Vercel deployment via CLI
```

#### Setup Instructions

**1. Railway Setup (Backend)**

- Sign up at [railway.app](https://railway.app)
- Create new project â†’ Connect GitHub repo
- Configure:
  - Root Directory: `backend`
  - Builder: `Dockerfile`
  - Add environment variables (see below)
- Generate domain in Settings â†’ Networking

**2. Vercel Setup (Frontend)**

```bash
cd frontend
vercel login
vercel link
```

Get credentials from `.vercel/project.json`:
- `orgId` â†’ `VERCEL_ORG_ID`
- `projectId` â†’ `VERCEL_PROJECT_ID`

Generate token: [vercel.com/account/tokens](https://vercel.com/account/tokens)

**3. GitHub Secrets**

Add these secrets in **Settings â†’ Secrets and variables â†’ Actions**:

```
VERCEL_TOKEN=xxx
VERCEL_ORG_ID=team_xxx
VERCEL_PROJECT_ID=prj_xxx
```

**4. Environment Variables**

**Railway (Backend):**
```bash
GEMINI_API_KEY=your_key

# Embedding Provider Configuration
EMBEDDING_PROVIDER=jina  # Options: jina, cohere, voyage, huggingface
JINA_API_KEY=your_key    # Required when EMBEDDING_PROVIDER=jina
# COHERE_API_KEY=your_key  # Required when EMBEDDING_PROVIDER=cohere
# VOYAGE_API_KEY=your_key  # Required when EMBEDDING_PROVIDER=voyage
# HF_API_KEY=your_key      # Required when EMBEDDING_PROVIDER=huggingface

QDRANT_URL=https://your-cluster.cloud.qdrant.io:6333
QDRANT_API_KEY=your_key
CORS_ORIGINS=https://your-app.vercel.app
ENVIRONMENT=production
```

**Vercel (Frontend):**
```bash
VITE_API_BASE=https://rag-stack-production.up.railway.app
```

---

## ğŸ“ Environment Variables

### Backend (`.env`)

```bash
# FastAPI Configuration
APP_NAME=RAG AI Backend
ENVIRONMENT=development
APP_PORT=8000

# Google Gemini
GEMINI_API_KEY=your_gemini_api_key

# Embedding Provider Configuration (Choose one)
EMBEDDING_PROVIDER=jina  # Options: jina, cohere, voyage, huggingface
JINA_API_KEY=your_jina_api_key    # Required when EMBEDDING_PROVIDER=jina
# COHERE_API_KEY=your_cohere_api_key  # Required when EMBEDDING_PROVIDER=cohere
# VOYAGE_API_KEY=your_voyage_api_key  # Required when EMBEDDING_PROVIDER=voyage
# HF_API_KEY=your_hf_api_key          # Required when EMBEDDING_PROVIDER=huggingface
JINA_MODEL_NAME=jina-embeddings-v3  # Used when EMBEDDING_PROVIDER=jina
# Alternative models if needed:
# EMBEDDING_MODEL_NAME=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
# EMBEDDING_MODEL_NAME=intfloat/multilingual-e5-small
# Alternative models if the above doesn't work: 
# EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
# EMBEDDING_MODEL_NAME=intfloat/multilingual-e5-small

# Qdrant
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=

# CORS
CORS_ORIGINS=http://localhost:5173,http://localhost:3000

# Chunking
CHUNK_SIZE=1000
CHUNK_OVERLAP=100
```

### Frontend (`.env`)

```bash
# API Configuration
VITE_API_BASE=http://localhost:8000

# Port
VITE_PORT=5173
```

---

## ğŸ§  RAG API Endpoints

### 1ï¸âƒ£ Health Check

```bash
curl http://localhost:8000/health
```

Response:
```json
{"status": "ok"}
```

### 2ï¸âƒ£ Index Content

```bash
curl -X POST "http://localhost:8000/rag/index" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "content=FastAPI is a modern Python web framework for building APIs."
```

Response:
```json
{
  "status": "indexed",
  "detail": {
    "chunks_indexed": 1
  }
}
```

### 3ï¸âƒ£ Query Knowledge Base

```bash
curl -X POST "http://localhost:8000/rag/query" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "query=What is FastAPI?"
```

Response:
```json
{
  "query": "What is FastAPI?",
  "answer": "FastAPI is a modern Python web framework designed for building APIs quickly and efficiently...",
  "context_used": ["FastAPI is a modern Python web framework..."]
}
```

---

## ğŸ§ª Testing

### Backend Tests

```bash
cd backend
pytest tests/ -v
```

### Frontend Tests

```bash
cd frontend
npm run lint
npm run build
```

---

## ğŸ¯ Key Features

### Smart Path-Based Deployment

The CI/CD pipeline intelligently detects changes:

- **Backend changes** â†’ Tests + Railway auto-deploy
- **Frontend changes** â†’ Tests + Vercel deploy
- **Root changes** â†’ Run all tests

This saves GitHub Actions minutes and speeds up deployment!

### Preview Deployments

Pull requests automatically get preview deployments:
- Frontend preview on Vercel
- Comments in PR with preview URL

---

## ğŸ§© Tech Stack

| Layer          | Technology                    |
| -------------- | ----------------------------- |
| Backend        | FastAPI, Python 3.11          |
| Frontend       | React 18, TypeScript, Vite    |
| AI/ML          | LangChain, Google Gemini      |
| Embeddings     | Multiple Providers (Jina AI, Cohere, Voyage AI, HuggingFace) |
| Vector DB      | Qdrant Cloud                  |
| Deployment     | Railway (backend), Vercel     |
| CI/CD          | GitHub Actions                |
| Containerization | Docker                      |

---

## ğŸ”„ RAG Pipeline Flow

| Stage          | Component       | Description                                              |
| -------------- | --------------- | -------------------------------------------------------- |
| 1ï¸âƒ£ Chunking   | LangChain       | Intelligent text splitting (size + overlap configurable) |
| 2ï¸âƒ£ Embedding  | Multiple Providers (Jina AI Default) | Generate vector embeddings using selected provider |
| 3ï¸âƒ£ Storage    | Qdrant Cloud    | Store embeddings and metadata                            |
| 4ï¸âƒ£ Query      | Semantic Search | Retrieve contextually similar chunks                     |
| 5ï¸âƒ£ Generation | Gemini          | Compose human-like, context-aware answers                |

---

## ğŸš€ Deployment Checklist

- [ ] Set up Railway account & project
- [ ] Configure Railway environment variables
- [ ] Set up Vercel account & link project
- [ ] Add GitHub secrets (Vercel tokens)
- [ ] Update `BACKEND_URL` in workflow
- [ ] Update `VITE_API_BASE` in Vercel settings
- [ ] Test local development
- [ ] Push to main â†’ Verify deployments
- [ ] Create PR â†’ Test preview deployment

---

## ğŸ“Š Monitoring

- **Railway:** Built-in logs and metrics dashboard
- **Vercel:** Analytics and deployment logs
- **GitHub Actions:** Workflow runs and job details

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¨â€ğŸ’» Author

**[Risdy](https://linkedin.com/in/rizts)**  
Remote Software Engineer (since 2013)  
AI & Fullstack Developer â€” FastAPI | React | LangChain | Gemini | Jina | Qdrant  
ğŸ“ Based in Indonesia

---

## ğŸ™ Acknowledgments

- Original concept: [rag-ai-concept](https://github.com/rizts/rag-ai-concept)
- Built with modern AI/ML frameworks
- Deployed with cloud-native infrastructure

---

**â­ If you find this project useful, please give it a star!**

â¤ï¸ Happy coding!