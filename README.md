[![CI](https://github.com/rizts/rag-stack/actions/workflows/deploy.yml/badge.svg)](https://github.com/rizts/rag-stack/actions/workflows/deploy.yml)
[![Deploy to Railway](https://github.com/rizts/rag-stack/actions/workflows/deploy.yml/badge.svg)](https://github.com/rizts/rag-stack/actions/workflows/deploy.yml)
[![Deploy to Vercel](https://github.com/rizts/rag-stack/actions/workflows/deploy.yml/badge.svg)](https://github.com/rizts/rag-stack/actions/workflows/deploy.yml)


# ğŸ§  AI Knowledge Service â€“ RAG & FastAPI

This project is a **Refactored and Production-Ready version** of the original [rizts/rag-ai-concept](https://github.com/rizts/rag-ai-concept).  
It aims to be more structured, modular, and ready for real-world deployment, with a clear separation between **Backend (FastAPI)** and **Frontend (Vite + ReactJS)**.


## ğŸš€ Overview

This project demonstrates a complete **AI pipeline** with:
- ğŸ§© **Backend (FastAPI)** â€” API layer, Gemini integration, and LangChain-based chunking.
- âš›ï¸ **Frontend (Vite + React + TypeScript)** â€” interactive RAG chat UI.
- ğŸ§  **AI Layer** â€” intelligent chunking, Gemini embeddings, semantic retrieval, and answer generation.
- ğŸ’¾ **Vector Database (Qdrant)** â€” document storage and vector similarity search.
- ğŸš€ **CI/CD** â€” Railway (backend) + Vercel (frontend).


The goal is to demonstrate a production-level **Retrieval-Augmented Generation (RAG)** system that can:
1. Process documents and intelligently chunk them using **LangChain**.
2. Generate embeddings and store them in **Qdrant** Vector Database.
3. Expose APIs for semantic search and knowledge retrieval.
4. Integrate with MLOps stages to showcase AI orchestration lifecycle.

---

## ğŸ—ï¸ Architecture Overview

```mermaid
graph TD
  A[ğŸ“„ Document Upload] --> B[ğŸ” Intelligent Chunking (LangChain)]
  B --> C[ğŸ”¢ Gemini Embeddings]
  C --> D[(ğŸ§  Qdrant Vector DB)]
  E[ğŸ’¬ Query Request] --> F[Semantic Retrieval + Contextual Search]
  F --> G[ğŸ§  Gemini Generative Response]
  G --> H[ğŸ’¡ Answer Output (React UI)]
  ```

## ğŸ—ï¸ Project Setup

### 1. Install dependencies
```bash
# Backend
# Create virtual environment (recommended)
cd backend
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Frontend
cd frontend
# Install dependencies
npm install
```

### 2 Run Vector Database (Qdrant)

```bash
cd backend/docker
docker compose up -d
```
Qdrant will run at http://localhost:6333

### 3. Run server

## Backend
```bash
# Backend
cd backend
uvicorn app.main:app --reload
```
Backend will run at http://localhost:8000


## Frontend
```bash
# Frontend
cd frontend
npm run dev
```
Frontend will run at http://localhost:5173


### 4. Check backend health endpoint
Open http://localhost:8000/health


### 5. Test the Integration

Open http://localhost:5173
Ask a question related to your indexed documents.
Answer will be generated based on Qdrant context retrieval.


### **Project Structure**
```bash
rag-stack/
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ app
â”‚   â”‚   â”œâ”€â”€ api
â”‚   â”‚   â”‚   â”œâ”€â”€ health.py
â”‚   â”‚   â”‚   â””â”€â”€ rag.py
â”‚   â”‚   â”œâ”€â”€ core
â”‚   â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â”‚   â””â”€â”€ logger.py
â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”œâ”€â”€ services
â”‚   â”‚   â”‚   â”œâ”€â”€ embeddings_huggingface.py
â”‚   â”‚   â”‚   â”œâ”€â”€ semantic.py
â”‚   â”‚   â”‚   â””â”€â”€ vectorstore_qdrant.py
â”‚   â”‚   â””â”€â”€ utils
â”‚   â”‚       â””â”€â”€ chunking.py
â”‚   â”œâ”€â”€ docker
â”‚   â”‚   â””â”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ Makefile
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”œâ”€â”€ rag_stack.egg-info
â”‚   â”‚   â”œâ”€â”€ dependency_links.txt
â”‚   â”‚   â”œâ”€â”€ PKG-INFO
â”‚   â”‚   â”œâ”€â”€ requires.txt
â”‚   â”‚   â”œâ”€â”€ SOURCES.txt
â”‚   â”‚   â””â”€â”€ top_level.txt
â”‚   â”œâ”€â”€ requirements-dev.txt
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ tests
â”‚       â””â”€â”€ test_rag_basic.py
â”œâ”€â”€ frontend
â”‚   â”œâ”€â”€ eslint.config.js
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ package-lock.json
â”‚   â”œâ”€â”€ public
â”‚   â”‚   â””â”€â”€ vite.svg
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ src
â”‚   â”‚   â”œâ”€â”€ api
â”‚   â”‚   â”‚   â””â”€â”€ rag.ts
â”‚   â”‚   â”œâ”€â”€ App.css
â”‚   â”‚   â”œâ”€â”€ App.tsx
â”‚   â”‚   â”œâ”€â”€ assets
â”‚   â”‚   â”‚   â””â”€â”€ react.svg
â”‚   â”‚   â”œâ”€â”€ components
â”‚   â”‚   â”‚   â”œâ”€â”€ AnswerCard.tsx
â”‚   â”‚   â”‚   â””â”€â”€ QueryForm.tsx
â”‚   â”‚   â”œâ”€â”€ index.css
â”‚   â”‚   â””â”€â”€ main.tsx
â”‚   â”œâ”€â”€ tsconfig.app.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â”œâ”€â”€ tsconfig.node.json
â”‚   â””â”€â”€ vite.config.ts
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

### ğŸ” Environment Variables

Backend `.env.example`
```bash
# === FastAPI Configuration ===
APP_NAME=HuggingFace RAG API
APP_ENV=development
APP_PORT=8000

# === Google Gemini ===
GEMINI_API_KEY=your_google_gemini_api_key

# === HuggingFace ===
HF_API_KEY=your_hf_api_key
EMBEDDING_MODEL_NAME=sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2

# === Qdrant ===
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=

# === Frontend ===
CORS_ORIGINS=["http://localhost:3000","http://127.0.0.1:3000"]

# === Railway ===
RAILWAY_TOKEN=your_railway_token

# === LangChain / Chunking Settings ===
CHUNK_SIZE=1000
CHUNK_OVERLAP=100

# === Deployment (Railway) ===
# Railway automatically injects PORT env var
# You can override below if running locally
PORT=8000
```

Frontend `.env.example`
```bash
# === Frontend Environment Variables ===
# Base URL of backend API (FastAPI on Railway)
VITE_API_BASE=http://localhost:8000

# Port for local development
VITE_PORT=5173
```

ğŸ§© Structure Overview

- `app/api` â†’ FastAPI routers
- `app/services` â†’ Core business logic (semantic search, vector store)
- `app/utils` â†’ Helper utilities (LangChain chunking)
- `app/core` â†’ Config & logging


## ğŸ§  RAG API Endpoints

### 1ï¸âƒ£ Index new content
```bash
curl -X POST "http://localhost:8000/rag/index" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "content=FastAPI is a Python framework for building APIs quickly."
```

Response:
```json
{"status": "indexed", "detail": {"chunks_indexed": 1}}
```

### 2ï¸âƒ£ Query the knowledge base
```bash
curl -X POST "http://localhost:8000/rag/query" \
  -H "Content-Type: application/x-www-form-urlencoded" \
  -d "query=What is FastAPI used for?"
```

Response:
```json
{
  "query": "What is FastAPI used for?",
  "answer": "FastAPI is used to build APIs quickly and efficiently in Python.",
  "context_used": ["FastAPI is a Python framework for building APIs quickly."]
}
```

### ğŸš€ Deployment
ğŸ”¹ Backend â†’ Railway
Configured via `backend/.github/workflows/deploy-backend.yml`
```yaml
name: Deploy Backend to Railway

on:
  push:
    branches:
      - main
    paths:
      - "backend/**"

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Deploy to Railway
        uses: railwayapp/railway-action@v2
        with:
          railwayToken: ${{ secrets.RAILWAY_TOKEN }}
          projectId: ${{ secrets.RAILWAY_PROJECT_ID }}
          service: backend
          path: ./backend
```

ğŸ”¹ Frontend â†’ Vercel
Configured via `frontend/.github/workflows/deploy-frontend.yml`
```yaml
name: Deploy Frontend to Vercel

on:
  push:
    branches:
      - main
    paths:
      - "frontend/**"

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Deploy to Vercel
        uses: amondnet/vercel-action@v25
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          vercel-org-id: ${{ secrets.VERCEL_ORG_ID }}
          vercel-project-id: ${{ secrets.VERCEL_PROJECT_ID }}
          working-directory: ./frontend
          prod: true
```

Both deployments run automatically on push to the main branch.


### ğŸ§  AI Orchestration (RAG Flow)

| Stage          | Component       | Description                                              |
| -------------- | --------------- | -------------------------------------------------------- |
| 1ï¸âƒ£ Chunking   | LangChain       | Intelligent text splitting (size + overlap configurable) |
| 2ï¸âƒ£ Embedding  | Gemini          | Generate vector embeddings via Gemini API                |
| 3ï¸âƒ£ Storage    | Qdrant          | Store embeddings and metadata                            |
| 4ï¸âƒ£ Query      | Semantic Search | Retrieve contextually similar chunks                     |
| 5ï¸âƒ£ Generation | Gemini          | Compose human-like, context-aware answers                |


### ğŸ§ª CI/CD Overview

| Project  | Workflow              | Description         |
| -------- | --------------------- | ------------------- |
| Backend  | `ci-backend.yml`      | Lint + test FastAPI |
| Backend  | `deploy-backend.yml`  | Deploy to Railway   |
| Frontend | `ci-frontend.yml`     | Lint + build React  |
| Frontend | `deploy-frontend.yml` | Deploy to Vercel    |


### ğŸ‘¨â€ğŸ’» Author

[Risdy](https://linkedin.com/in/rizts)
Remote Software Engineer (since 2013)
AI & Fullstack Developer â€” FastAPI | React | LangChain | Gemini | HuggingFace | Qdrant
ğŸ“ Based in Indonesia

### ğŸ **License**

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
Feel free to fork and build your own RAG experiments.

â¤ï¸ Happy coding!